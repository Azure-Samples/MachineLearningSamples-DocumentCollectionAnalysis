{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Learning of Key Phrases and Topics in Document Collections\n",
    "\n",
    "## Part 2: Phrase Learning\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is Part 2 of 6, in a series providing a step-by-step description of how to process and analyze the contents of a large collection of text documents in an unsupervised manner. Using Python packages and custom code examples, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.\n",
    "\n",
    "Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range of natural language data including news stories, legal documents, research publications, social media forum discussions, customer feedback forms, product reviews, and many more.\n",
    "\n",
    "Part 2 of the series shows how to learn the most salient phrases present in a large collection of documents. These phrases can be treated as single compound word units in down-stream processes such as topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import re\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from math import log\n",
    "from sys import getsizeof\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import platform\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFrame = pandas.read_csv('../Data/CongressionalDocsCleaned.tsv', sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in cleaned text: 5310574\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>1</td>\n",
       "      <td>States that it shall be the duty of the commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>2</td>\n",
       "      <td>and to report to the Congress its findings in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>3</td>\n",
       "      <td>Authorizes the committee to make such expendit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>4</td>\n",
       "      <td>hold such hearings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>5</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText\n",
       "0   hconres1-93        0             Provides that effective from January 3\n",
       "1   hconres1-93        1                                               1973\n",
       "2   hconres1-93        2  the joint committee created to make the necess...\n",
       "3   hconres1-93        3  is hereby continued and for such purpose shall...\n",
       "4   hconres1-93        4                      of the Ninety-second Congress\n",
       "5   hconres2-93        0  Makes it the sense of the Congress that the po...\n",
       "6   hconres2-93        1  Makes it the sense of the Congress that the Pr...\n",
       "7   hconres2-93        2  acting through the United States delegation to...\n",
       "8   hconres2-93        3  should take such steps as may be necessary to ...\n",
       "9   hconres2-93        4  or amendments to existing international agreem...\n",
       "10  hconres2-93        5                              as may be appropriate\n",
       "11  hconres2-93        6  providing for coordinated international activi...\n",
       "12  hconres2-93        7                                          chemicals\n",
       "13  hconres2-93        8                                 chemical munitions\n",
       "14  hconres2-93        9                                  military material\n",
       "15  hconres2-93       10           and any pollutants in territorial waters\n",
       "16  hconres2-93       11                                   contiguous zones\n",
       "17  hconres2-93       12        the deep seabed or any international waters\n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...\n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...\n",
       "20  hconres3-93        1  States that it shall be the duty of the commit...\n",
       "21  hconres3-93        2  and to report to the Congress its findings in ...\n",
       "22  hconres3-93        3  Authorizes the committee to make such expendit...\n",
       "23  hconres3-93        4                                 hold such hearings\n",
       "24  hconres3-93        5            and employ such assistants as necessary"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Total lines in cleaned text: %d\\n\" % len(textFrame))\n",
    "\n",
    "# Show the first 25 rows of the data in the frame\n",
    "textFrame[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lowercase Version of the Text Data\n",
    "\n",
    "Before learning phrases we lowercase the entire text corpus to ensure all casing variants for each word are collapsed into a single uniform variant used during the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "      <td>provides that effective from january 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "      <td>of the ninety-second congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "      <td>makes it the sense of the congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "      <td>makes it the sense of the congress that the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "      <td>acting through the united states delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "      <td>chemical munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "      <td>establishes a joint congressional committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>1</td>\n",
       "      <td>States that it shall be the duty of the commit...</td>\n",
       "      <td>states that it shall be the duty of the commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>2</td>\n",
       "      <td>and to report to the Congress its findings in ...</td>\n",
       "      <td>and to report to the congress its findings in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>3</td>\n",
       "      <td>Authorizes the committee to make such expendit...</td>\n",
       "      <td>authorizes the committee to make such expendit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>4</td>\n",
       "      <td>hold such hearings</td>\n",
       "      <td>hold such hearings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>5</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText  \\\n",
       "0   hconres1-93        0             Provides that effective from January 3   \n",
       "1   hconres1-93        1                                               1973   \n",
       "2   hconres1-93        2  the joint committee created to make the necess...   \n",
       "3   hconres1-93        3  is hereby continued and for such purpose shall...   \n",
       "4   hconres1-93        4                      of the Ninety-second Congress   \n",
       "5   hconres2-93        0  Makes it the sense of the Congress that the po...   \n",
       "6   hconres2-93        1  Makes it the sense of the Congress that the Pr...   \n",
       "7   hconres2-93        2  acting through the United States delegation to...   \n",
       "8   hconres2-93        3  should take such steps as may be necessary to ...   \n",
       "9   hconres2-93        4  or amendments to existing international agreem...   \n",
       "10  hconres2-93        5                              as may be appropriate   \n",
       "11  hconres2-93        6  providing for coordinated international activi...   \n",
       "12  hconres2-93        7                                          chemicals   \n",
       "13  hconres2-93        8                                 chemical munitions   \n",
       "14  hconres2-93        9                                  military material   \n",
       "15  hconres2-93       10           and any pollutants in territorial waters   \n",
       "16  hconres2-93       11                                   contiguous zones   \n",
       "17  hconres2-93       12        the deep seabed or any international waters   \n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...   \n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...   \n",
       "20  hconres3-93        1  States that it shall be the duty of the commit...   \n",
       "21  hconres3-93        2  and to report to the Congress its findings in ...   \n",
       "22  hconres3-93        3  Authorizes the committee to make such expendit...   \n",
       "23  hconres3-93        4                                 hold such hearings   \n",
       "24  hconres3-93        5            and employ such assistants as necessary   \n",
       "\n",
       "                                        LowercaseText  \n",
       "0              provides that effective from january 3  \n",
       "1                                                1973  \n",
       "2   the joint committee created to make the necess...  \n",
       "3   is hereby continued and for such purpose shall...  \n",
       "4                       of the ninety-second congress  \n",
       "5   makes it the sense of the congress that the po...  \n",
       "6   makes it the sense of the congress that the pr...  \n",
       "7   acting through the united states delegation to...  \n",
       "8   should take such steps as may be necessary to ...  \n",
       "9   or amendments to existing international agreem...  \n",
       "10                              as may be appropriate  \n",
       "11  providing for coordinated international activi...  \n",
       "12                                          chemicals  \n",
       "13                                 chemical munitions  \n",
       "14                                  military material  \n",
       "15           and any pollutants in territorial waters  \n",
       "16                                   contiguous zones  \n",
       "17        the deep seabed or any international waters  \n",
       "18  and otherwise to prevent the pollution of the ...  \n",
       "19  establishes a joint congressional committee on...  \n",
       "20  states that it shall be the duty of the commit...  \n",
       "21  and to report to the congress its findings in ...  \n",
       "22  authorizes the committee to make such expendit...  \n",
       "23                                 hold such hearings  \n",
       "24            and employ such assistants as necessary  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lowercased version of the data and add it into the data frame\n",
    "lowercaseText = []\n",
    "for textLine in textFrame['CleanedText']:\n",
    "    lowercaseText.append(str(textLine).lower())\n",
    "textFrame['LowercaseText'] = lowercaseText;           \n",
    "            \n",
    "textFrame[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Supplemental Word Lists\n",
    "\n",
    "Words in the black list are completely ignored by the process and cannot be used in the creation of phrases. Words in the function word list can only be used in between content words in the creation of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for loading lists into dictionary hash tables\n",
    "def LoadListAsHash(filename):\n",
    "    listHash = {}\n",
    "    fp = open(filename, encoding='utf-8')\n",
    "\n",
    "    # Read in lines one by one stripping away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in fp.readlines():\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "\n",
    "    fp.close()\n",
    "    return listHash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the black list of words to ignore \n",
    "blacklistHash = LoadListAsHash('../Data/black_list.txt')\n",
    "\n",
    "# Load the list of non-content bearing function words\n",
    "functionwordHash = LoadListAsHash('../Data/function_words.txt')\n",
    "\n",
    "# Add more terms to the function word list\n",
    "functionwordHash[\"foo\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute N-gram Statistics for Phrase Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that used to define how to compute Ngram stats\n",
    "# This function will be executed in parallel as process pool executor\n",
    "def ComputeNgramStatsJob(textList, functionwordHash, blacklistHash, reValidWord, jobId, verbose=False):\n",
    "    if verbose:\n",
    "        startTS = datetime.now()\n",
    "        print(\"[%s] Starting batch execution %d\" % (str(startTS), jobId+1))\n",
    "    \n",
    "    # Create an array to store the total count of all ngrams up to 4-grams\n",
    "    # Array element 0 is unused, element 1 is unigrams, element 2 is bigrams, etc.\n",
    "    ngramCounts = [0]*5;\n",
    "       \n",
    "    # Create a list of structures to tabulate ngram count statistics\n",
    "    # Array element 0 is the array of total ngram counts,\n",
    "    # Array element 1 is a hash table of individual unigram counts\n",
    "    # Array element 2 is a hash table of individual bigram counts\n",
    "    # Array element 3 is a hash table of individual trigram counts\n",
    "    # Array element 4 is a hash table of individual 4-gram counts\n",
    "    ngramStats = [ngramCounts, {}, {}, {}, {}]\n",
    "    \n",
    "    numLines = len(textList)\n",
    "    if verbose:\n",
    "        print(\"# Batch %d, received %d lines data\" % (jobId+1, numLines))\n",
    "    \n",
    "    for i in range(0, numLines):\n",
    "        # Split the text line into an array of words\n",
    "        wordArray = textList[i].strip().split()\n",
    "        numWords = len(wordArray)\n",
    "        \n",
    "        # Create an array marking each word as valid or invalid\n",
    "        validArray = [reValidWord.match(word) != None for word in wordArray]\n",
    "        \n",
    "        # Tabulate total raw ngrams for this line into counts for each ngram bin\n",
    "        # The total ngrams counts include the counts of all ngrams including those\n",
    "        # that we won't consider as parts of phrases\n",
    "        for j in range(1, 5):\n",
    "            if j <= numWords:\n",
    "                ngramCounts[j] += numWords - j + 1\n",
    "        \n",
    "        # Collect counts for viable phrase ngrams and left context sub-phrases\n",
    "        for j in range(0, numWords):\n",
    "            word = wordArray[j]\n",
    "\n",
    "            # Only bother counting the ngrams that start with a valid content word\n",
    "            # i.e., valids words not in the function word list or the black list\n",
    "            if ( ( word not in functionwordHash ) and ( word not in blacklistHash ) and validArray[j] ):\n",
    "\n",
    "                # Initialize ngram string with first content word and add it to unigram counts\n",
    "                ngramSeq = word \n",
    "                if ngramSeq in ngramStats[1]:\n",
    "                    ngramStats[1][ngramSeq] += 1\n",
    "                else:\n",
    "                    ngramStats[1][ngramSeq] = 1\n",
    "\n",
    "                # Count valid ngrams from bigrams up to 4-grams\n",
    "                stop = 0\n",
    "                k = 1\n",
    "                while (k<4) and (j+k<numWords) and not stop:\n",
    "                    n = k + 1\n",
    "                    nextNgramWord = wordArray[j+k]\n",
    "                    # Only count ngrams with valid words not in the blacklist\n",
    "                    if ( validArray[j+k] and nextNgramWord not in blacklistHash ):\n",
    "                        ngramSeq += \" \" + nextNgramWord\n",
    "                        if ngramSeq in ngramStats[n]:\n",
    "                            ngramStats[n][ngramSeq] += 1\n",
    "                        else:\n",
    "                            ngramStats[n][ngramSeq] = 1 \n",
    "                        k += 1\n",
    "                        if nextNgramWord not in functionwordHash:\n",
    "                            # Stop counting new ngrams after second content word in \n",
    "                            # ngram is reached and ngram is a viable full phrase\n",
    "                            stop = 1\n",
    "                    else:\n",
    "                        stop = 1\n",
    "    \n",
    "    if verbose:\n",
    "        endTS = datetime.now()\n",
    "        delta_t = (endTS - startTS).total_seconds()\n",
    "        print(\"[%s] Batch %d finished, time elapsed: %f seconds\" % (str(endTS), jobId+1, delta_t))\n",
    "    \n",
    "    return ngramStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is Step 1 for each iteration of phrase learning\n",
    "# We count the number of occurances of all 2-gram, 3-ngram, and 4-gram\n",
    "# word sequences \n",
    "def ComputeNgramStats(textData, functionwordHash, blacklistHash, numWorkers, verbose=False):\n",
    "          \n",
    "    # Create a regular expression for assessing validity of words\n",
    "    # for phrase modeling. The expression says words in phrases\n",
    "    # must either:\n",
    "    # (1) contain an alphabetic character, or \n",
    "    # (2) be the single charcater '&', or\n",
    "    # (3) be a one or two digit number\n",
    "    reWordIsValid = re.compile('[A-Za-z]|^&$|^\\d\\d?$');\n",
    "    \n",
    "    # Go through the text data line by line collecting count statistics\n",
    "    # for all valid n-grams that could appear in a potential phrase\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # Get the number of CPU to run the tasks\n",
    "    if numWorkers > cpu_count() or numWorkers <= 0:\n",
    "        worker = cpu_count()\n",
    "    else:\n",
    "        worker = numWorkers\n",
    "    if verbose:\n",
    "        print(\"Worker size = %d\" % worker)\n",
    "    \n",
    "    # Get the batch size for each execution job\n",
    "    # The very last job executor may received more lines of data\n",
    "    batch_size = int(numLines/worker)\n",
    "    batchIndexes = range(0, numLines, batch_size)\n",
    "    \n",
    "    batch_returns = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=worker) as executor:\n",
    "        jobs = set()\n",
    "        \n",
    "        # Map the task into multiple batch executions\n",
    "        if platform.system() == \"Linux\" or platform.system() == \"Darwin\":\n",
    "            for idx in range(worker):\n",
    "                # The very last job executor\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(executor.submit(ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]: ], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx, \n",
    "                                                 verbose))\n",
    "                else:\n",
    "                    jobs.add(executor.submit(ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx,\n",
    "                                                 verbose))\n",
    "        else:\n",
    "            # For Windows system, it is different to handle ProcessPoolExecutor\n",
    "            import winprocess\n",
    "            \n",
    "            for idx in range(worker):\n",
    "                # The very last job executor\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                                 ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]: ], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx, \n",
    "                                                 verbose))\n",
    "                else:\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                                 ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx,\n",
    "                                                 verbose))\n",
    "        \n",
    "        # Get results from batch executions\n",
    "        for job in concurrent.futures.as_completed(jobs):\n",
    "            try:\n",
    "                ret = job.result()\n",
    "            except Exception as e:\n",
    "                print(\"Generated an exception while trying to get result from a batch: %s\" % e)\n",
    "            else:\n",
    "                batch_returns.append(ret)\n",
    "\n",
    "    # Reduce the results from batche executions\n",
    "    # Reuse the first return\n",
    "    ngramStats = batch_returns[0]\n",
    "    \n",
    "    for batch_id in range(1, len(batch_returns)):\n",
    "        result = batch_returns[batch_id]\n",
    "        \n",
    "        # Update the ngram counts\n",
    "        ngramStats[0] = [x + y for x, y in zip(ngramStats[0], result[0])]\n",
    "        \n",
    "        # Update the hash table of ngram counts\n",
    "        for n_gram in range(1, 5):\n",
    "            for item in result[n_gram]:\n",
    "                if item in ngramStats[n_gram]:\n",
    "                    ngramStats[n_gram][item] += result[n_gram][item]\n",
    "                else:\n",
    "                    ngramStats[n_gram][item] = result[n_gram][item]\n",
    "    \n",
    "    return ngramStats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Potential Phrases by the Weighted Pointwise Mutual Information of their Constituent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankNgrams(ngramStats,functionwordHash,minCount):\n",
    "    # Create a hash table to store weighted pointwise mutual \n",
    "    # information scores for each viable phrase\n",
    "    ngramWPMIHash = {}\n",
    "        \n",
    "    # Go through each of the ngram tables and compute the phrase scores\n",
    "    # for the viable phrases\n",
    "    for n in range(2,5):\n",
    "        i = n-1\n",
    "        for ngram in ngramStats[n].keys():\n",
    "            ngramCount = ngramStats[n][ngram]\n",
    "            if ngramCount >= minCount:\n",
    "                wordArray = ngram.split()\n",
    "                # If the final word in the ngram is not a function word then\n",
    "                # the ngram is a valid phrase candidate we want to score\n",
    "                if wordArray[i] not in functionwordHash: \n",
    "                    leftNgram = ' '.join(wordArray[:-1])\n",
    "                    rightWord = wordArray[i]\n",
    "                    \n",
    "                    # Compute the weighted pointwise mutual information (WPMI) for the phrase\n",
    "                    probNgram = float(ngramStats[n][ngram])/float(ngramStats[0][n])\n",
    "                    probLeftNgram = float(ngramStats[n-1][leftNgram])/float(ngramStats[0][n-1])\n",
    "                    probRightWord = float(ngramStats[1][rightWord])/float(ngramStats[0][1])\n",
    "                    WPMI = probNgram * math.log(probNgram/(probLeftNgram*probRightWord));\n",
    "\n",
    "                    # Add the phrase into the list of scored phrases only if WMPI is positive\n",
    "                    if WPMI > 0:\n",
    "                        ngramWPMIHash[ngram] = WPMI  \n",
    "    \n",
    "    # Create a sorted list of the phrase candidates\n",
    "    rankedNgrams = sorted(ngramWPMIHash, key=ngramWPMIHash.__getitem__, reverse=True)\n",
    "\n",
    "    # Force a memory clean-up\n",
    "    ngramWPMIHash = None\n",
    "    gc.collect()\n",
    "\n",
    "    return rankedNgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Phrase Rewrites to Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phraseRewriteJob(ngramRegex, text, ngramRewriteHash, jobId, verbose=True):\n",
    "    if verbose:\n",
    "        startTS = datetime.now()\n",
    "        print(\"[%s] Starting batch execution %d\" % (str(startTS), jobId+1))\n",
    "    \n",
    "    retList = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        # The regex substituion looks up the output string rewrite\n",
    "        # in the hash table for each matched input phrase regex\n",
    "        retList.append(ngramRegex.sub(lambda mo: ngramRewriteHash[mo.string[mo.start():mo.end()]], text[i]))\n",
    "    \n",
    "    if verbose:\n",
    "        endTS = datetime.now()\n",
    "        delta_t = (endTS - startTS).total_seconds()\n",
    "        print(\"[%s] Batch %d finished, batch size: %d, time elapsed: %f seconds\" % (str(endTS), jobId+1, i, delta_t))\n",
    "    \n",
    "    return retList, jobId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewrites(rankedNgrams, textData, learnedPhrases, maxPhrasesToAdd, \n",
    "                        maxPhraseLength, verbose, numWorkers=cpu_count()):\n",
    "\n",
    "    # If the number of rankedNgrams coming in is zero then\n",
    "    # just return without doing anything\n",
    "    numNgrams = len(rankedNgrams)\n",
    "    if numNgrams == 0:\n",
    "        return\n",
    "\n",
    "    # This function will consider at most maxRewrite \n",
    "    # new phrases to be added into the learned phrase \n",
    "    # list as specified by the calling function\n",
    "    maxRewrite=maxPhrasesToAdd\n",
    "\n",
    "    # If the remaining number of proposed ngram phrases is less \n",
    "    # than the max allowed, then reset maxRewrite to the size of \n",
    "    # the proposed ngram phrases list\n",
    "    if numNgrams < maxRewrite:\n",
    "        maxRewrite = numNgrams\n",
    "\n",
    "    # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "    leftConflictHash = {}\n",
    "    rightConflictHash = {}\n",
    "    \n",
    "    # Create an empty hash table collecting the set of rewrite rules\n",
    "    # to be applied during this iteration of phrase learning\n",
    "    ngramRewriteHash = {}\n",
    "    \n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "\n",
    "    # Initialize some bookkeeping variables\n",
    "    numLines = len(textData)  \n",
    "    numPhrasesAdded = 0\n",
    "    numConsidered = 0\n",
    "    lastSkippedNgram = \"\"\n",
    "    lastAddedNgram = \"\"\n",
    "  \n",
    "    # Collect list of up to maxRewrite ngram phrase rewrites\n",
    "    stop = False\n",
    "    index = 0\n",
    "    while not stop:\n",
    "\n",
    "        # Get the next phrase to consider adding to the phrase list\n",
    "        inputNgram = rankedNgrams[index]\n",
    "\n",
    "        # Create the output compound word version of the phrase\n",
    "        # The extra space is added to make the regex rewrite easier\n",
    "        outputNgram = \" \" + regexSpace.sub(\"_\",inputNgram)\n",
    "\n",
    "        # Count the total number of words in the proposed phrase\n",
    "        numWords = len(outputNgram.split(\"_\"))\n",
    "\n",
    "        # Only add phrases that don't exceed the max phrase length\n",
    "        if (numWords <= maxPhraseLength):\n",
    "    \n",
    "            # Keep count of phrases considered for inclusion during this iteration\n",
    "            numConsidered += 1\n",
    "\n",
    "            # Extract the left and right words in the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = inputNgram.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[-1]\n",
    "\n",
    "            # Skip any ngram phrases that conflict with earlier phrases added\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if (leftWord in leftConflictHash) or (rightWord in rightConflictHash): \n",
    "                if verbose: \n",
    "                    print (\"(%d) Skipping (context conflict): %s\" % (numConsidered,inputNgram))\n",
    "                lastSkippedNgram = inputNgram\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                if verbose:\n",
    "                    print (\"(%d) Adding: %s\" % (numConsidered,inputNgram))\n",
    "                ngramRewriteHash[\" \" + inputNgram] = outputNgram\n",
    "                learnedPhrases.append(inputNgram) \n",
    "                lastAddedNgram = inputNgram\n",
    "                numPhrasesAdded += 1\n",
    "            \n",
    "            # Keep track of all context words that might conflict with upcoming\n",
    "            # propose phrases (even when phrases are skipped instead of added)\n",
    "            leftConflictHash[rightWord] = 1\n",
    "            rightConflictHash[leftWord] = 1\n",
    "\n",
    "            # Stop when we've considered the maximum number of phrases per iteration\n",
    "            if ( numConsidered >= maxRewrite ):\n",
    "                stop = True\n",
    "            \n",
    "        # Increment to next phrase\n",
    "        index += 1\n",
    "    \n",
    "        # Stop if we've reached the end of the ranked ngram list\n",
    "        if index >= len(rankedNgrams):\n",
    "            stop = True\n",
    "    \n",
    "    # Now do the phrase rewrites over the entire set of text data\n",
    "    # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "    ngramRegex = re.compile(r'%s(?= )' % \"(?= )|\".join(map(re.escape, ngramRewriteHash.keys())))\n",
    "    \n",
    "    # Get the number of CPU to run the tasks\n",
    "    if numWorkers > cpu_count() or numWorkers <= 0:\n",
    "        worker = cpu_count()\n",
    "    else:\n",
    "        worker = numWorkers\n",
    "    if verbose:\n",
    "        print(\"Worker size = %d\" % worker)\n",
    "        \n",
    "    # Get the batch size for each execution job\n",
    "    # The very last job executor may received more lines of data\n",
    "    batch_size = int(numLines/worker)\n",
    "    batchIndexes = range(0, numLines, batch_size)\n",
    "    \n",
    "    batch_returns = [[]] * worker\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        jobs = set()\n",
    "        \n",
    "        # Map the task into multiple batch executions\n",
    "        if platform.system() == \"Linux\" or platform.system() == \"Darwin\":\n",
    "            for idx in range(worker):\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(executor.submit(phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]: ], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "                else:\n",
    "                    jobs.add(executor.submit(phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "        else:\n",
    "            import winprocess\n",
    "            \n",
    "            for idx in range(worker):\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                             phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]: ], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "                else:\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                             phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "        \n",
    "        textData.clear()\n",
    "        \n",
    "        # Get results from batch executions\n",
    "        for job in concurrent.futures.as_completed(jobs):\n",
    "            try:\n",
    "                ret, idx = job.result()\n",
    "            except Exception as e:\n",
    "                print(\"Generated an exception while trying to get result from a batch: %s\" % e)\n",
    "            else:\n",
    "                batch_returns[idx] = ret\n",
    "        textData += sum(batch_returns, [])\n",
    "     \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the full iterative phrase learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseLearning(textData,learnedPhrases,learningSettings):\n",
    "    \n",
    "    stop = False\n",
    "    iterNum = 0\n",
    "\n",
    "    # Get the learning parameters from the structue passed in by the calling function\n",
    "    maxNumPhrases = learningSettings.maxNumPhrases\n",
    "    maxPhraseLength = learningSettings.maxPhraseLength\n",
    "    functionwordHash = learningSettings.functionwordHash\n",
    "    blacklistHash = learningSettings.blacklistHash\n",
    "    verbose = learningSettings.verbose\n",
    "    minCount = learningSettings.minInstanceCount\n",
    "    \n",
    "    # Start timing the process\n",
    "    functionStartTime = time.clock()\n",
    "    \n",
    "    numPhrasesLearned = len(learnedPhrases)\n",
    "    print (\"Start phrase learning with %d phrases of %d phrases learned\" % (numPhrasesLearned,maxNumPhrases))\n",
    "\n",
    "    while not stop:\n",
    "        iterNum += 1\n",
    "                \n",
    "        # Start timing this iteration\n",
    "        startTime = time.clock()\n",
    " \n",
    "        # Collect ngram stats\n",
    "        ngramStats = ComputeNgramStats(textData, functionwordHash, blacklistHash, cpu_count()-1, verbose)\n",
    "\n",
    "        # Uncomment this for more detailed timing info\n",
    "        countTime = time.clock()\n",
    "        elapsedTime = countTime - startTime\n",
    "        print (\"--- Counting time: %.2f seconds\" % elapsedTime)\n",
    "        \n",
    "        # Rank ngrams\n",
    "        rankedNgrams = RankNgrams(ngramStats,functionwordHash,minCount)\n",
    "        \n",
    "        # Uncomment this for more detailed timing info\n",
    "        rankTime = time.clock()\n",
    "        elapsedTime = rankTime - countTime\n",
    "        print (\"--- Ranking time: %.2f seconds\" % elapsedTime)\n",
    "        \n",
    "        \n",
    "        # Incorporate top ranked phrases into phrase list\n",
    "        # and rewrite the text to use these phrases\n",
    "        if len(rankedNgrams) > 0:\n",
    "            maxPhrasesToAdd = maxNumPhrases - numPhrasesLearned\n",
    "            if maxPhrasesToAdd > learningSettings.maxPhrasesPerIter:\n",
    "                maxPhrasesToAdd = learningSettings.maxPhrasesPerIter\n",
    "            ApplyPhraseRewrites(rankedNgrams, textData, learnedPhrases, maxPhrasesToAdd, \n",
    "                                maxPhraseLength, verbose, cpu_count()-1)\n",
    "            numPhrasesAdded = len(learnedPhrases) - numPhrasesLearned\n",
    "        else:\n",
    "            stop = True\n",
    "            \n",
    "        # Uncomment this for more detailed timing info\n",
    "        rewriteTime = time.clock()\n",
    "        elapsedTime = rewriteTime - rankTime\n",
    "        print (\"--- Rewriting time: %.2f seconds\" % elapsedTime)\n",
    "           \n",
    "        # Garbage collect\n",
    "        ngramStats = None\n",
    "        rankedNgrams = None\n",
    "        gc.collect();\n",
    "               \n",
    "        elapsedTime = time.clock() - startTime\n",
    "\n",
    "        numPhrasesLearned = len(learnedPhrases)\n",
    "        print (\"Iteration %d: Added %d new phrases in %.2f seconds (Learned %d of max %d)\" % \n",
    "               (iterNum,numPhrasesAdded,elapsedTime,numPhrasesLearned,maxNumPhrases))\n",
    "        \n",
    "        if numPhrasesAdded >= maxPhrasesToAdd or numPhrasesAdded == 0:\n",
    "            stop = True\n",
    "        \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    gc.collect()\n",
    " \n",
    "    elapsedTime = time.clock() - functionStartTime\n",
    "    elapsedTimeHours = elapsedTime/3600.0;\n",
    "    print (\"*** Phrase learning completed in %.2f hours ***\" % elapsedTimeHours) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Main top level execution of phrase learning functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 25000 phrases learned\n",
      "--- Counting time: 46.98 seconds\n",
      "--- Ranking time: 7.59 seconds\n",
      "--- Rewriting time: 27.83 seconds\n",
      "Iteration 1: Added 242 new phrases in 83.84 seconds (Learned 242 of max 25000)\n",
      "--- Counting time: 46.83 seconds\n",
      "--- Ranking time: 7.74 seconds\n",
      "--- Rewriting time: 24.74 seconds\n",
      "Iteration 2: Added 221 new phrases in 80.81 seconds (Learned 463 of max 25000)\n",
      "--- Counting time: 46.36 seconds\n",
      "--- Ranking time: 7.60 seconds\n",
      "--- Rewriting time: 23.72 seconds\n",
      "Iteration 3: Added 206 new phrases in 79.44 seconds (Learned 669 of max 25000)\n",
      "--- Counting time: 43.71 seconds\n",
      "--- Ranking time: 7.53 seconds\n",
      "--- Rewriting time: 22.00 seconds\n",
      "Iteration 4: Added 194 new phrases in 74.63 seconds (Learned 863 of max 25000)\n",
      "--- Counting time: 41.64 seconds\n",
      "--- Ranking time: 7.07 seconds\n",
      "--- Rewriting time: 18.09 seconds\n",
      "Iteration 5: Added 169 new phrases in 68.16 seconds (Learned 1032 of max 25000)\n",
      "--- Counting time: 41.19 seconds\n",
      "--- Ranking time: 6.71 seconds\n",
      "--- Rewriting time: 15.61 seconds\n",
      "Iteration 6: Added 138 new phrases in 64.84 seconds (Learned 1170 of max 25000)\n",
      "--- Counting time: 39.92 seconds\n",
      "--- Ranking time: 6.70 seconds\n",
      "--- Rewriting time: 15.09 seconds\n",
      "Iteration 7: Added 121 new phrases in 63.08 seconds (Learned 1291 of max 25000)\n",
      "--- Counting time: 40.48 seconds\n",
      "--- Ranking time: 6.80 seconds\n",
      "--- Rewriting time: 15.46 seconds\n",
      "Iteration 8: Added 124 new phrases in 64.23 seconds (Learned 1415 of max 25000)\n",
      "--- Counting time: 41.12 seconds\n",
      "--- Ranking time: 7.59 seconds\n",
      "--- Rewriting time: 16.06 seconds\n",
      "Iteration 9: Added 118 new phrases in 66.21 seconds (Learned 1533 of max 25000)\n",
      "--- Counting time: 43.69 seconds\n",
      "--- Ranking time: 8.11 seconds\n",
      "--- Rewriting time: 16.93 seconds\n",
      "Iteration 10: Added 132 new phrases in 70.22 seconds (Learned 1665 of max 25000)\n",
      "--- Counting time: 43.85 seconds\n",
      "--- Ranking time: 7.62 seconds\n",
      "--- Rewriting time: 18.39 seconds\n",
      "Iteration 11: Added 151 new phrases in 71.56 seconds (Learned 1816 of max 25000)\n",
      "--- Counting time: 45.24 seconds\n",
      "--- Ranking time: 7.69 seconds\n",
      "--- Rewriting time: 17.66 seconds\n",
      "Iteration 12: Added 152 new phrases in 72.23 seconds (Learned 1968 of max 25000)\n",
      "--- Counting time: 44.94 seconds\n",
      "--- Ranking time: 8.22 seconds\n",
      "--- Rewriting time: 18.07 seconds\n",
      "Iteration 13: Added 147 new phrases in 72.88 seconds (Learned 2115 of max 25000)\n",
      "--- Counting time: 45.87 seconds\n",
      "--- Ranking time: 8.02 seconds\n",
      "--- Rewriting time: 18.15 seconds\n",
      "Iteration 14: Added 148 new phrases in 73.65 seconds (Learned 2263 of max 25000)\n",
      "--- Counting time: 48.20 seconds\n",
      "--- Ranking time: 8.06 seconds\n",
      "--- Rewriting time: 18.40 seconds\n",
      "Iteration 15: Added 140 new phrases in 76.23 seconds (Learned 2403 of max 25000)\n",
      "--- Counting time: 45.65 seconds\n",
      "--- Ranking time: 8.27 seconds\n",
      "--- Rewriting time: 17.05 seconds\n",
      "Iteration 16: Added 131 new phrases in 73.28 seconds (Learned 2534 of max 25000)\n",
      "--- Counting time: 45.34 seconds\n",
      "--- Ranking time: 7.58 seconds\n",
      "--- Rewriting time: 16.73 seconds\n",
      "Iteration 17: Added 139 new phrases in 71.17 seconds (Learned 2673 of max 25000)\n",
      "--- Counting time: 43.89 seconds\n",
      "--- Ranking time: 7.82 seconds\n",
      "--- Rewriting time: 16.98 seconds\n",
      "Iteration 18: Added 127 new phrases in 70.45 seconds (Learned 2800 of max 25000)\n",
      "--- Counting time: 44.00 seconds\n",
      "--- Ranking time: 7.62 seconds\n",
      "--- Rewriting time: 15.50 seconds\n",
      "Iteration 19: Added 124 new phrases in 68.55 seconds (Learned 2924 of max 25000)\n",
      "--- Counting time: 42.79 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 16.28 seconds\n",
      "Iteration 20: Added 135 new phrases in 68.59 seconds (Learned 3059 of max 25000)\n",
      "--- Counting time: 42.92 seconds\n",
      "--- Ranking time: 7.94 seconds\n",
      "--- Rewriting time: 16.49 seconds\n",
      "Iteration 21: Added 130 new phrases in 68.83 seconds (Learned 3189 of max 25000)\n",
      "--- Counting time: 43.59 seconds\n",
      "--- Ranking time: 7.80 seconds\n",
      "--- Rewriting time: 16.31 seconds\n",
      "Iteration 22: Added 129 new phrases in 69.28 seconds (Learned 3318 of max 25000)\n",
      "--- Counting time: 43.19 seconds\n",
      "--- Ranking time: 7.60 seconds\n",
      "--- Rewriting time: 17.89 seconds\n",
      "Iteration 23: Added 152 new phrases in 70.14 seconds (Learned 3470 of max 25000)\n",
      "--- Counting time: 44.94 seconds\n",
      "--- Ranking time: 7.49 seconds\n",
      "--- Rewriting time: 17.27 seconds\n",
      "Iteration 24: Added 147 new phrases in 71.29 seconds (Learned 3617 of max 25000)\n",
      "--- Counting time: 43.29 seconds\n",
      "--- Ranking time: 7.90 seconds\n",
      "--- Rewriting time: 18.12 seconds\n",
      "Iteration 25: Added 144 new phrases in 70.94 seconds (Learned 3761 of max 25000)\n",
      "--- Counting time: 45.44 seconds\n",
      "--- Ranking time: 8.02 seconds\n",
      "--- Rewriting time: 17.73 seconds\n",
      "Iteration 26: Added 148 new phrases in 72.83 seconds (Learned 3909 of max 25000)\n",
      "--- Counting time: 45.03 seconds\n",
      "--- Ranking time: 8.00 seconds\n",
      "--- Rewriting time: 18.05 seconds\n",
      "Iteration 27: Added 149 new phrases in 72.90 seconds (Learned 4058 of max 25000)\n",
      "--- Counting time: 46.89 seconds\n",
      "--- Ranking time: 8.43 seconds\n",
      "--- Rewriting time: 20.80 seconds\n",
      "Iteration 28: Added 172 new phrases in 77.97 seconds (Learned 4230 of max 25000)\n",
      "--- Counting time: 46.30 seconds\n",
      "--- Ranking time: 8.48 seconds\n",
      "--- Rewriting time: 19.08 seconds\n",
      "Iteration 29: Added 159 new phrases in 75.96 seconds (Learned 4389 of max 25000)\n",
      "--- Counting time: 48.39 seconds\n",
      "--- Ranking time: 7.98 seconds\n",
      "--- Rewriting time: 20.31 seconds\n",
      "Iteration 30: Added 160 new phrases in 78.45 seconds (Learned 4549 of max 25000)\n",
      "--- Counting time: 42.70 seconds\n",
      "--- Ranking time: 7.93 seconds\n",
      "--- Rewriting time: 17.43 seconds\n",
      "Iteration 31: Added 153 new phrases in 69.63 seconds (Learned 4702 of max 25000)\n",
      "--- Counting time: 39.85 seconds\n",
      "--- Ranking time: 7.11 seconds\n",
      "--- Rewriting time: 15.12 seconds\n",
      "Iteration 32: Added 137 new phrases in 63.45 seconds (Learned 4839 of max 25000)\n",
      "--- Counting time: 39.59 seconds\n",
      "--- Ranking time: 7.56 seconds\n",
      "--- Rewriting time: 14.79 seconds\n",
      "Iteration 33: Added 125 new phrases in 63.42 seconds (Learned 4964 of max 25000)\n",
      "--- Counting time: 40.36 seconds\n",
      "--- Ranking time: 7.14 seconds\n",
      "--- Rewriting time: 14.31 seconds\n",
      "Iteration 34: Added 112 new phrases in 63.45 seconds (Learned 5076 of max 25000)\n",
      "--- Counting time: 40.08 seconds\n",
      "--- Ranking time: 7.74 seconds\n",
      "--- Rewriting time: 15.39 seconds\n",
      "Iteration 35: Added 118 new phrases in 64.79 seconds (Learned 5194 of max 25000)\n",
      "--- Counting time: 43.41 seconds\n",
      "--- Ranking time: 8.00 seconds\n",
      "--- Rewriting time: 14.41 seconds\n",
      "Iteration 36: Added 103 new phrases in 67.33 seconds (Learned 5297 of max 25000)\n",
      "--- Counting time: 43.66 seconds\n",
      "--- Ranking time: 8.04 seconds\n",
      "--- Rewriting time: 15.50 seconds\n",
      "Iteration 37: Added 112 new phrases in 68.77 seconds (Learned 5409 of max 25000)\n",
      "--- Counting time: 43.34 seconds\n",
      "--- Ranking time: 7.93 seconds\n",
      "--- Rewriting time: 14.91 seconds\n",
      "Iteration 38: Added 108 new phrases in 67.81 seconds (Learned 5517 of max 25000)\n",
      "--- Counting time: 44.21 seconds\n",
      "--- Ranking time: 7.98 seconds\n",
      "--- Rewriting time: 14.76 seconds\n",
      "Iteration 39: Added 100 new phrases in 68.57 seconds (Learned 5617 of max 25000)\n",
      "--- Counting time: 43.63 seconds\n",
      "--- Ranking time: 8.21 seconds\n",
      "--- Rewriting time: 15.51 seconds\n",
      "Iteration 40: Added 95 new phrases in 69.17 seconds (Learned 5712 of max 25000)\n",
      "--- Counting time: 46.13 seconds\n",
      "--- Ranking time: 7.82 seconds\n",
      "--- Rewriting time: 14.79 seconds\n",
      "Iteration 41: Added 106 new phrases in 70.37 seconds (Learned 5818 of max 25000)\n",
      "--- Counting time: 43.15 seconds\n",
      "--- Ranking time: 7.41 seconds\n",
      "--- Rewriting time: 14.74 seconds\n",
      "Iteration 42: Added 116 new phrases in 66.82 seconds (Learned 5934 of max 25000)\n",
      "--- Counting time: 42.98 seconds\n",
      "--- Ranking time: 7.50 seconds\n",
      "--- Rewriting time: 16.46 seconds\n",
      "Iteration 43: Added 143 new phrases in 68.50 seconds (Learned 6077 of max 25000)\n",
      "--- Counting time: 42.58 seconds\n",
      "--- Ranking time: 7.35 seconds\n",
      "--- Rewriting time: 15.56 seconds\n",
      "Iteration 44: Added 131 new phrases in 67.05 seconds (Learned 6208 of max 25000)\n",
      "--- Counting time: 41.84 seconds\n",
      "--- Ranking time: 7.50 seconds\n",
      "--- Rewriting time: 15.20 seconds\n",
      "Iteration 45: Added 126 new phrases in 66.05 seconds (Learned 6334 of max 25000)\n",
      "--- Counting time: 42.46 seconds\n",
      "--- Ranking time: 7.49 seconds\n",
      "--- Rewriting time: 14.45 seconds\n",
      "Iteration 46: Added 111 new phrases in 65.98 seconds (Learned 6445 of max 25000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting time: 42.06 seconds\n",
      "--- Ranking time: 7.45 seconds\n",
      "--- Rewriting time: 14.87 seconds\n",
      "Iteration 47: Added 117 new phrases in 65.91 seconds (Learned 6562 of max 25000)\n",
      "--- Counting time: 42.43 seconds\n",
      "--- Ranking time: 7.49 seconds\n",
      "--- Rewriting time: 15.99 seconds\n",
      "Iteration 48: Added 131 new phrases in 67.46 seconds (Learned 6693 of max 25000)\n",
      "--- Counting time: 41.52 seconds\n",
      "--- Ranking time: 7.50 seconds\n",
      "--- Rewriting time: 19.09 seconds\n",
      "Iteration 49: Added 174 new phrases in 69.64 seconds (Learned 6867 of max 25000)\n",
      "--- Counting time: 41.72 seconds\n",
      "--- Ranking time: 7.51 seconds\n",
      "--- Rewriting time: 20.40 seconds\n",
      "Iteration 50: Added 194 new phrases in 71.18 seconds (Learned 7061 of max 25000)\n",
      "--- Counting time: 41.72 seconds\n",
      "--- Ranking time: 7.43 seconds\n",
      "--- Rewriting time: 21.32 seconds\n",
      "Iteration 51: Added 204 new phrases in 72.02 seconds (Learned 7265 of max 25000)\n",
      "--- Counting time: 41.48 seconds\n",
      "--- Ranking time: 7.59 seconds\n",
      "--- Rewriting time: 22.16 seconds\n",
      "Iteration 52: Added 215 new phrases in 72.81 seconds (Learned 7480 of max 25000)\n",
      "--- Counting time: 42.78 seconds\n",
      "--- Ranking time: 7.90 seconds\n",
      "--- Rewriting time: 22.67 seconds\n",
      "Iteration 53: Added 216 new phrases in 74.88 seconds (Learned 7696 of max 25000)\n",
      "--- Counting time: 41.46 seconds\n",
      "--- Ranking time: 7.61 seconds\n",
      "--- Rewriting time: 24.41 seconds\n",
      "Iteration 54: Added 242 new phrases in 74.96 seconds (Learned 7938 of max 25000)\n",
      "--- Counting time: 41.83 seconds\n",
      "--- Ranking time: 7.80 seconds\n",
      "--- Rewriting time: 24.86 seconds\n",
      "Iteration 55: Added 255 new phrases in 76.08 seconds (Learned 8193 of max 25000)\n",
      "--- Counting time: 42.28 seconds\n",
      "--- Ranking time: 7.60 seconds\n",
      "--- Rewriting time: 25.82 seconds\n",
      "Iteration 56: Added 261 new phrases in 77.25 seconds (Learned 8454 of max 25000)\n",
      "--- Counting time: 41.57 seconds\n",
      "--- Ranking time: 7.44 seconds\n",
      "--- Rewriting time: 25.21 seconds\n",
      "Iteration 57: Added 257 new phrases in 75.84 seconds (Learned 8711 of max 25000)\n",
      "--- Counting time: 42.19 seconds\n",
      "--- Ranking time: 7.69 seconds\n",
      "--- Rewriting time: 23.77 seconds\n",
      "Iteration 58: Added 236 new phrases in 75.17 seconds (Learned 8947 of max 25000)\n",
      "--- Counting time: 41.77 seconds\n",
      "--- Ranking time: 7.45 seconds\n",
      "--- Rewriting time: 22.99 seconds\n",
      "Iteration 59: Added 223 new phrases in 73.82 seconds (Learned 9170 of max 25000)\n",
      "--- Counting time: 41.21 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 20.76 seconds\n",
      "Iteration 60: Added 198 new phrases in 71.22 seconds (Learned 9368 of max 25000)\n",
      "--- Counting time: 41.92 seconds\n",
      "--- Ranking time: 7.88 seconds\n",
      "--- Rewriting time: 19.51 seconds\n",
      "Iteration 61: Added 183 new phrases in 70.84 seconds (Learned 9551 of max 25000)\n",
      "--- Counting time: 41.40 seconds\n",
      "--- Ranking time: 7.75 seconds\n",
      "--- Rewriting time: 21.27 seconds\n",
      "Iteration 62: Added 203 new phrases in 71.96 seconds (Learned 9754 of max 25000)\n",
      "--- Counting time: 41.89 seconds\n",
      "--- Ranking time: 7.96 seconds\n",
      "--- Rewriting time: 18.72 seconds\n",
      "Iteration 63: Added 179 new phrases in 70.17 seconds (Learned 9933 of max 25000)\n",
      "--- Counting time: 42.00 seconds\n",
      "--- Ranking time: 7.65 seconds\n",
      "--- Rewriting time: 20.00 seconds\n",
      "Iteration 64: Added 192 new phrases in 71.34 seconds (Learned 10125 of max 25000)\n",
      "--- Counting time: 41.67 seconds\n",
      "--- Ranking time: 7.56 seconds\n",
      "--- Rewriting time: 23.54 seconds\n",
      "Iteration 65: Added 236 new phrases in 74.36 seconds (Learned 10361 of max 25000)\n",
      "--- Counting time: 42.33 seconds\n",
      "--- Ranking time: 7.62 seconds\n",
      "--- Rewriting time: 23.80 seconds\n",
      "Iteration 66: Added 243 new phrases in 75.36 seconds (Learned 10604 of max 25000)\n",
      "--- Counting time: 41.03 seconds\n",
      "--- Ranking time: 7.64 seconds\n",
      "--- Rewriting time: 25.76 seconds\n",
      "Iteration 67: Added 270 new phrases in 76.04 seconds (Learned 10874 of max 25000)\n",
      "--- Counting time: 41.23 seconds\n",
      "--- Ranking time: 7.60 seconds\n",
      "--- Rewriting time: 24.58 seconds\n",
      "Iteration 68: Added 248 new phrases in 75.11 seconds (Learned 11122 of max 25000)\n",
      "--- Counting time: 41.15 seconds\n",
      "--- Ranking time: 7.69 seconds\n",
      "--- Rewriting time: 22.08 seconds\n",
      "Iteration 69: Added 217 new phrases in 72.44 seconds (Learned 11339 of max 25000)\n",
      "--- Counting time: 40.53 seconds\n",
      "--- Ranking time: 7.58 seconds\n",
      "--- Rewriting time: 24.83 seconds\n",
      "Iteration 70: Added 251 new phrases in 74.48 seconds (Learned 11590 of max 25000)\n",
      "--- Counting time: 40.83 seconds\n",
      "--- Ranking time: 7.87 seconds\n",
      "--- Rewriting time: 24.28 seconds\n",
      "Iteration 71: Added 249 new phrases in 74.51 seconds (Learned 11839 of max 25000)\n",
      "--- Counting time: 41.05 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 23.06 seconds\n",
      "Iteration 72: Added 237 new phrases in 73.37 seconds (Learned 12076 of max 25000)\n",
      "--- Counting time: 41.89 seconds\n",
      "--- Ranking time: 7.71 seconds\n",
      "--- Rewriting time: 23.99 seconds\n",
      "Iteration 73: Added 252 new phrases in 75.17 seconds (Learned 12328 of max 25000)\n",
      "--- Counting time: 41.51 seconds\n",
      "--- Ranking time: 7.63 seconds\n",
      "--- Rewriting time: 24.51 seconds\n",
      "Iteration 74: Added 252 new phrases in 75.25 seconds (Learned 12580 of max 25000)\n",
      "--- Counting time: 40.94 seconds\n",
      "--- Ranking time: 7.69 seconds\n",
      "--- Rewriting time: 24.58 seconds\n",
      "Iteration 75: Added 255 new phrases in 74.84 seconds (Learned 12835 of max 25000)\n",
      "--- Counting time: 41.26 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 24.71 seconds\n",
      "Iteration 76: Added 259 new phrases in 75.19 seconds (Learned 13094 of max 25000)\n",
      "--- Counting time: 41.12 seconds\n",
      "--- Ranking time: 7.69 seconds\n",
      "--- Rewriting time: 25.74 seconds\n",
      "Iteration 77: Added 269 new phrases in 76.09 seconds (Learned 13363 of max 25000)\n",
      "--- Counting time: 41.04 seconds\n",
      "--- Ranking time: 7.80 seconds\n",
      "--- Rewriting time: 25.83 seconds\n",
      "Iteration 78: Added 266 new phrases in 76.32 seconds (Learned 13629 of max 25000)\n",
      "--- Counting time: 41.99 seconds\n",
      "--- Ranking time: 7.60 seconds\n",
      "--- Rewriting time: 23.85 seconds\n",
      "Iteration 79: Added 246 new phrases in 75.02 seconds (Learned 13875 of max 25000)\n",
      "--- Counting time: 41.14 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 23.73 seconds\n",
      "Iteration 80: Added 251 new phrases in 74.17 seconds (Learned 14126 of max 25000)\n",
      "--- Counting time: 41.47 seconds\n",
      "--- Ranking time: 7.64 seconds\n",
      "--- Rewriting time: 24.05 seconds\n",
      "Iteration 81: Added 247 new phrases in 74.74 seconds (Learned 14373 of max 25000)\n",
      "--- Counting time: 40.61 seconds\n",
      "--- Ranking time: 7.85 seconds\n",
      "--- Rewriting time: 26.79 seconds\n",
      "Iteration 82: Added 280 new phrases in 76.84 seconds (Learned 14653 of max 25000)\n",
      "--- Counting time: 40.91 seconds\n",
      "--- Ranking time: 7.80 seconds\n",
      "--- Rewriting time: 28.83 seconds\n",
      "Iteration 83: Added 312 new phrases in 79.11 seconds (Learned 14965 of max 25000)\n",
      "--- Counting time: 40.85 seconds\n",
      "--- Ranking time: 7.63 seconds\n",
      "--- Rewriting time: 29.40 seconds\n",
      "Iteration 84: Added 323 new phrases in 79.47 seconds (Learned 15288 of max 25000)\n",
      "--- Counting time: 41.19 seconds\n",
      "--- Ranking time: 7.96 seconds\n",
      "--- Rewriting time: 29.21 seconds\n",
      "Iteration 85: Added 321 new phrases in 79.92 seconds (Learned 15609 of max 25000)\n",
      "--- Counting time: 40.88 seconds\n",
      "--- Ranking time: 7.74 seconds\n",
      "--- Rewriting time: 29.09 seconds\n",
      "Iteration 86: Added 320 new phrases in 79.30 seconds (Learned 15929 of max 25000)\n",
      "--- Counting time: 41.46 seconds\n",
      "--- Ranking time: 7.77 seconds\n",
      "--- Rewriting time: 27.64 seconds\n",
      "Iteration 87: Added 303 new phrases in 78.56 seconds (Learned 16232 of max 25000)\n",
      "--- Counting time: 41.00 seconds\n",
      "--- Ranking time: 7.80 seconds\n",
      "--- Rewriting time: 28.47 seconds\n",
      "Iteration 88: Added 304 new phrases in 78.84 seconds (Learned 16536 of max 25000)\n",
      "--- Counting time: 40.71 seconds\n",
      "--- Ranking time: 7.70 seconds\n",
      "--- Rewriting time: 27.46 seconds\n",
      "Iteration 89: Added 289 new phrases in 77.42 seconds (Learned 16825 of max 25000)\n",
      "--- Counting time: 41.22 seconds\n",
      "--- Ranking time: 7.88 seconds\n",
      "--- Rewriting time: 25.01 seconds\n",
      "Iteration 90: Added 265 new phrases in 75.79 seconds (Learned 17090 of max 25000)\n",
      "--- Counting time: 41.82 seconds\n",
      "--- Ranking time: 7.67 seconds\n",
      "--- Rewriting time: 24.93 seconds\n",
      "Iteration 91: Added 263 new phrases in 76.05 seconds (Learned 17353 of max 25000)\n",
      "--- Counting time: 41.11 seconds\n",
      "--- Ranking time: 7.76 seconds\n",
      "--- Rewriting time: 27.39 seconds\n",
      "Iteration 92: Added 290 new phrases in 77.84 seconds (Learned 17643 of max 25000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting time: 40.83 seconds\n",
      "--- Ranking time: 7.71 seconds\n",
      "--- Rewriting time: 28.13 seconds\n",
      "Iteration 93: Added 304 new phrases in 78.27 seconds (Learned 17947 of max 25000)\n",
      "--- Counting time: 41.60 seconds\n",
      "--- Ranking time: 7.71 seconds\n",
      "--- Rewriting time: 29.33 seconds\n",
      "Iteration 94: Added 318 new phrases in 80.25 seconds (Learned 18265 of max 25000)\n",
      "--- Counting time: 40.67 seconds\n",
      "--- Ranking time: 7.94 seconds\n",
      "--- Rewriting time: 28.36 seconds\n",
      "Iteration 95: Added 310 new phrases in 78.58 seconds (Learned 18575 of max 25000)\n",
      "--- Counting time: 40.82 seconds\n",
      "--- Ranking time: 7.84 seconds\n",
      "--- Rewriting time: 29.90 seconds\n",
      "Iteration 96: Added 334 new phrases in 80.20 seconds (Learned 18909 of max 25000)\n",
      "--- Counting time: 41.61 seconds\n",
      "--- Ranking time: 7.68 seconds\n",
      "--- Rewriting time: 29.63 seconds\n",
      "Iteration 97: Added 327 new phrases in 80.54 seconds (Learned 19236 of max 25000)\n",
      "--- Counting time: 40.51 seconds\n",
      "--- Ranking time: 7.72 seconds\n",
      "--- Rewriting time: 29.58 seconds\n",
      "Iteration 98: Added 323 new phrases in 79.43 seconds (Learned 19559 of max 25000)\n",
      "--- Counting time: 40.78 seconds\n",
      "--- Ranking time: 7.72 seconds\n",
      "--- Rewriting time: 28.96 seconds\n",
      "Iteration 99: Added 315 new phrases in 79.07 seconds (Learned 19874 of max 25000)\n",
      "--- Counting time: 41.31 seconds\n",
      "--- Ranking time: 8.06 seconds\n",
      "--- Rewriting time: 29.95 seconds\n",
      "Iteration 100: Added 334 new phrases in 80.91 seconds (Learned 20208 of max 25000)\n",
      "--- Counting time: 41.36 seconds\n",
      "--- Ranking time: 7.90 seconds\n",
      "--- Rewriting time: 30.88 seconds\n",
      "Iteration 101: Added 348 new phrases in 81.77 seconds (Learned 20556 of max 25000)\n",
      "--- Counting time: 40.97 seconds\n",
      "--- Ranking time: 7.78 seconds\n",
      "--- Rewriting time: 29.47 seconds\n",
      "Iteration 102: Added 328 new phrases in 79.95 seconds (Learned 20884 of max 25000)\n",
      "--- Counting time: 41.45 seconds\n",
      "--- Ranking time: 7.95 seconds\n",
      "--- Rewriting time: 28.65 seconds\n",
      "Iteration 103: Added 313 new phrases in 79.68 seconds (Learned 21197 of max 25000)\n",
      "--- Counting time: 40.94 seconds\n",
      "--- Ranking time: 7.81 seconds\n",
      "--- Rewriting time: 28.99 seconds\n",
      "Iteration 104: Added 316 new phrases in 79.34 seconds (Learned 21513 of max 25000)\n",
      "--- Counting time: 40.79 seconds\n",
      "--- Ranking time: 7.74 seconds\n",
      "--- Rewriting time: 30.50 seconds\n",
      "Iteration 105: Added 342 new phrases in 80.74 seconds (Learned 21855 of max 25000)\n",
      "--- Counting time: 41.03 seconds\n",
      "--- Ranking time: 7.91 seconds\n",
      "--- Rewriting time: 30.12 seconds\n",
      "Iteration 106: Added 337 new phrases in 80.67 seconds (Learned 22192 of max 25000)\n",
      "--- Counting time: 41.11 seconds\n",
      "--- Ranking time: 7.76 seconds\n",
      "--- Rewriting time: 31.00 seconds\n",
      "Iteration 107: Added 337 new phrases in 81.52 seconds (Learned 22529 of max 25000)\n",
      "--- Counting time: 41.55 seconds\n",
      "--- Ranking time: 7.77 seconds\n",
      "--- Rewriting time: 29.36 seconds\n",
      "Iteration 108: Added 324 new phrases in 80.36 seconds (Learned 22853 of max 25000)\n",
      "--- Counting time: 41.14 seconds\n",
      "--- Ranking time: 7.93 seconds\n",
      "--- Rewriting time: 29.76 seconds\n",
      "Iteration 109: Added 328 new phrases in 80.44 seconds (Learned 23181 of max 25000)\n",
      "--- Counting time: 41.72 seconds\n",
      "--- Ranking time: 7.86 seconds\n",
      "--- Rewriting time: 28.79 seconds\n",
      "Iteration 110: Added 320 new phrases in 80.00 seconds (Learned 23501 of max 25000)\n",
      "--- Counting time: 40.99 seconds\n",
      "--- Ranking time: 7.72 seconds\n",
      "--- Rewriting time: 27.29 seconds\n",
      "Iteration 111: Added 302 new phrases in 77.72 seconds (Learned 23803 of max 25000)\n",
      "--- Counting time: 40.86 seconds\n",
      "--- Ranking time: 7.86 seconds\n",
      "--- Rewriting time: 27.01 seconds\n",
      "Iteration 112: Added 295 new phrases in 77.36 seconds (Learned 24098 of max 25000)\n",
      "--- Counting time: 40.49 seconds\n",
      "--- Ranking time: 7.79 seconds\n",
      "--- Rewriting time: 26.07 seconds\n",
      "Iteration 113: Added 280 new phrases in 76.05 seconds (Learned 24378 of max 25000)\n",
      "--- Counting time: 40.54 seconds\n",
      "--- Ranking time: 8.04 seconds\n",
      "--- Rewriting time: 25.11 seconds\n",
      "Iteration 114: Added 270 new phrases in 75.37 seconds (Learned 24648 of max 25000)\n",
      "--- Counting time: 40.98 seconds\n",
      "--- Ranking time: 8.17 seconds\n",
      "--- Rewriting time: 15.43 seconds\n",
      "Iteration 115: Added 134 new phrases in 66.25 seconds (Learned 24782 of max 25000)\n",
      "--- Counting time: 41.00 seconds\n",
      "--- Ranking time: 7.77 seconds\n",
      "--- Rewriting time: 11.25 seconds\n",
      "Iteration 116: Added 50 new phrases in 61.69 seconds (Learned 24832 of max 25000)\n",
      "--- Counting time: 40.91 seconds\n",
      "--- Ranking time: 7.86 seconds\n",
      "--- Rewriting time: 10.34 seconds\n",
      "Iteration 117: Added 21 new phrases in 60.76 seconds (Learned 24853 of max 25000)\n",
      "--- Counting time: 40.95 seconds\n",
      "--- Ranking time: 8.06 seconds\n",
      "--- Rewriting time: 11.03 seconds\n",
      "Iteration 118: Added 40 new phrases in 61.74 seconds (Learned 24893 of max 25000)\n",
      "--- Counting time: 40.79 seconds\n",
      "--- Ranking time: 8.02 seconds\n",
      "--- Rewriting time: 10.31 seconds\n",
      "Iteration 119: Added 21 new phrases in 60.85 seconds (Learned 24914 of max 25000)\n",
      "--- Counting time: 41.46 seconds\n",
      "--- Ranking time: 7.78 seconds\n",
      "--- Rewriting time: 10.79 seconds\n",
      "Iteration 120: Added 32 new phrases in 61.63 seconds (Learned 24946 of max 25000)\n",
      "--- Counting time: 40.89 seconds\n",
      "--- Ranking time: 7.83 seconds\n",
      "--- Rewriting time: 10.15 seconds\n",
      "Iteration 121: Added 16 new phrases in 60.59 seconds (Learned 24962 of max 25000)\n",
      "--- Counting time: 41.61 seconds\n",
      "--- Ranking time: 14.76 seconds\n",
      "--- Rewriting time: 13.36 seconds\n",
      "Iteration 122: Added 10 new phrases in 71.61 seconds (Learned 24972 of max 25000)\n",
      "--- Counting time: 45.33 seconds\n",
      "--- Ranking time: 9.16 seconds\n",
      "--- Rewriting time: 11.32 seconds\n",
      "Iteration 123: Added 26 new phrases in 67.53 seconds (Learned 24998 of max 25000)\n",
      "--- Counting time: 47.85 seconds\n",
      "--- Ranking time: 8.98 seconds\n",
      "--- Rewriting time: 11.60 seconds\n",
      "Iteration 124: Added 2 new phrases in 70.39 seconds (Learned 25000 of max 25000)\n",
      "*** Phrase learning completed in 2.52 hours ***\n"
     ]
    }
   ],
   "source": [
    "# Create a structure defining the settings and word lists used during the phrase learning\n",
    "learningSettings = namedtuple('learningSettings',['maxNumPhrases','maxPhrasesPerIter',\n",
    "                                                  'maxPhraseLength','minInstanceCount'\n",
    "                                                  'functionwordHash','blacklistHash','verbose'])\n",
    "\n",
    "# If true it prints out the learned phrases to stdout buffer\n",
    "# while its learning. This will generate a lot of text to stdout, \n",
    "# so best to turn this off except for testing and debugging\n",
    "learningSettings.verbose = False\n",
    "\n",
    "# Maximium number of phrases to learn\n",
    "# If you want to test the code out quickly then set this to a small\n",
    "# value (e.g. 100) and set verbose to true when running the quick test\n",
    "learningSettings.maxNumPhrases = 25000\n",
    "\n",
    "# Maximum number of phrases to learn per iteration \n",
    "# Increasing this number may speed up processing but will affect the ordering of the phrases \n",
    "# learned and good phrases could be by-passed if the maxNumPhrases is set to a small number\n",
    "learningSettings.maxPhrasesPerIter = 500\n",
    "\n",
    "# Maximum number of words allowed in the learned phrases \n",
    "learningSettings.maxPhraseLength = 7\n",
    "\n",
    "# Minimum number of times a phrase must occur in the data to \n",
    "# be considered during the phrase learning process\n",
    "learningSettings.minInstanceCount = 5\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of function words used during phrase learning\n",
    "learningSettings.functionwordHash = functionwordHash\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of black list words to be ignored during phrase learning\n",
    "learningSettings.blacklistHash = blacklistHash\n",
    "\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrases = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextData = []\n",
    "for textLine in textFrame['LowercaseText']:\n",
    "    phraseTextData.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "if True:\n",
    "    ApplyPhraseLearning(phraseTextData, learnedPhrases, learningSettings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedPhrasesFile = \"../Data/CongressionalDocsLearnedPhrases.txt\"\n",
    "phraseTextDataFile = \"../Data/CongressionalDocsPhraseTextData.txt\"\n",
    "\n",
    "writeLearnedPhrases = True\n",
    "\n",
    "if writeLearnedPhrases:\n",
    "    # Write out the learned phrases to a text file\n",
    "    fp = open(learnedPhrasesFile, 'w', encoding='utf-8')\n",
    "    for phrase in learnedPhrases:\n",
    "        fp.write(\"%s\\n\" % phrase)\n",
    "    fp.close()\n",
    "\n",
    "    # Write out the text data containing the learned phrases to a text file\n",
    "    fp = open(phraseTextDataFile, 'w', encoding='utf-8')\n",
    "    for line in phraseTextData:\n",
    "        fp.write(\"%s\\n\" % line)\n",
    "    fp.close()\n",
    "else:\n",
    "    # Read in the learned phrases from a text file\n",
    "    learnedPhrases = []\n",
    "    fp = open(learnedPhrasesFile, 'r', encoding='utf-8')\n",
    "    for line in fp:\n",
    "        learnedPhrases.append(line.strip())\n",
    "    fp.close()\n",
    "\n",
    "    # Read in the learned phrases from a text file\n",
    "    phraseTextData = []\n",
    "    fp = open(phraseTextDataFile, 'r', encoding='utf-8')\n",
    "    for line in fp:\n",
    "        phraseTextData.append(line.strip())\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united states',\n",
       " 'directs the secretary',\n",
       " 'sets forth',\n",
       " 'internal revenue',\n",
       " 'fiscal year',\n",
       " 'authorizes the secretary',\n",
       " 'social security',\n",
       " 'authorizes appropriations',\n",
       " 'requires the secretary',\n",
       " 'expresses the sense']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnedPhrases[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spell of illness',\n",
       " 'military retired_pay',\n",
       " 'multipurpose_senior centers',\n",
       " 'drug_control and system improvement',\n",
       " 'cease to exist',\n",
       " 'contributions and expenditures',\n",
       " 'cost containment',\n",
       " 'paid from the contingent_fund',\n",
       " 'capacity building',\n",
       " 'north korean']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnedPhrases[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provides that effective from january_3',\n",
       " '1973',\n",
       " 'the joint_committee created to make the necessary arrangements for the inauguration of the president-elect_and_vice_president-elect of the united_states on the 20th day of january 1973',\n",
       " 'is hereby continued and for such purpose shall have the same power and authority as that conferred by senate concurrent_resolution 63',\n",
       " 'of the ninety-second congress',\n",
       " 'makes_it_the_sense_of_the_congress that the pollution of waters all over the world is a matter of vital concern to all_nations and should be dealt with as a matter of the highest_priority',\n",
       " 'makes_it_the_sense_of_the_congress that the president',\n",
       " 'acting through the united_states delegation to the united national_conference on the human_environment',\n",
       " 'should take such steps as may be necessary to propose an international_agreement',\n",
       " 'or amendments to existing international_agreements',\n",
       " 'as may be appropriate',\n",
       " 'providing for coordinated international activites to prohibit the disposal of munitions',\n",
       " 'chemicals',\n",
       " 'chemical_munitions',\n",
       " 'military material']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phraseTextData[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add text with learned phrases back into data frame\n",
    "textFrame['TextWithPhrases'] = phraseTextData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "      <th>TextWithPhrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "      <td>provides that effective from january 3</td>\n",
       "      <td>provides that effective from january_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "      <td>the joint_committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "      <td>of the ninety-second congress</td>\n",
       "      <td>of the ninety-second congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "      <td>makes it the sense of the congress that the po...</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "      <td>makes it the sense of the congress that the pr...</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "      <td>acting through the united states delegation to...</td>\n",
       "      <td>acting through the united_states delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "      <td>or amendments to existing international_agreem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DocID  DocLine                                        CleanedText  \\\n",
       "0  hconres1-93        0             Provides that effective from January 3   \n",
       "1  hconres1-93        1                                               1973   \n",
       "2  hconres1-93        2  the joint committee created to make the necess...   \n",
       "3  hconres1-93        3  is hereby continued and for such purpose shall...   \n",
       "4  hconres1-93        4                      of the Ninety-second Congress   \n",
       "5  hconres2-93        0  Makes it the sense of the Congress that the po...   \n",
       "6  hconres2-93        1  Makes it the sense of the Congress that the Pr...   \n",
       "7  hconres2-93        2  acting through the United States delegation to...   \n",
       "8  hconres2-93        3  should take such steps as may be necessary to ...   \n",
       "9  hconres2-93        4  or amendments to existing international agreem...   \n",
       "\n",
       "                                       LowercaseText  \\\n",
       "0             provides that effective from january 3   \n",
       "1                                               1973   \n",
       "2  the joint committee created to make the necess...   \n",
       "3  is hereby continued and for such purpose shall...   \n",
       "4                      of the ninety-second congress   \n",
       "5  makes it the sense of the congress that the po...   \n",
       "6  makes it the sense of the congress that the pr...   \n",
       "7  acting through the united states delegation to...   \n",
       "8  should take such steps as may be necessary to ...   \n",
       "9  or amendments to existing international agreem...   \n",
       "\n",
       "                                     TextWithPhrases  \n",
       "0             provides that effective from january_3  \n",
       "1                                               1973  \n",
       "2  the joint_committee created to make the necess...  \n",
       "3  is hereby continued and for such purpose shall...  \n",
       "4                      of the ninety-second congress  \n",
       "5  makes_it_the_sense_of_the_congress that the po...  \n",
       "6  makes_it_the_sense_of_the_congress that the pr...  \n",
       "7  acting through the united_states delegation to...  \n",
       "8  should take such steps as may be necessary to ...  \n",
       "9  or amendments to existing international_agreem...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFrame[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the joint_committee created to make the necessary arrangements for the inauguration of the president-elect_and_vice_president-elect of the united_states on the 20th day of january 1973'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFrame['TextWithPhrases'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most Common Surface Form of Each Lower-Cased Word and Phrase\n",
    "\n",
    "The text data is lower cased in order to merge differently cased versions of the same word prior to doing topic modeling. In order to generate summaries of topics that will be learned, we would like to present the most likely surface form of a word to the user. For example, if a proper noun is converted to all lowercase characters for latent topic modeling, we want the user to see this proper name with its proper capitalization within summaries. The MapVocabToSurfaceForms() function achieves this by mapping every lowercased word and phrase used during latent topic modeling to its most common surface form in the text collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapVocabToSurfaceForms(textData):\n",
    "    surfaceFormCountHash = {}\n",
    "    vocabToSurfaceFormHash = {}\n",
    "    regexUnderBar = re.compile('_')\n",
    "    regexSpace = re.compile(' +')\n",
    "    regexClean = re.compile('^ +| +$')\n",
    "    \n",
    "    # First go through every line of text, align each word/phrase with\n",
    "    # it's surface form and count the number of times each surface form occurs\n",
    "    for i in range(0,len(textData)):    \n",
    "        origWords = regexSpace.split(regexClean.sub(\"\",str(textData['CleanedText'][i])))\n",
    "        numOrigWords = len(origWords)\n",
    "        newWords = regexSpace.split(regexClean.sub(\"\",str(textData['TextWithPhrases'][i])))\n",
    "        numNewWords = len(newWords)\n",
    "        origIndex = 0\n",
    "        newIndex = 0\n",
    "        while newIndex < numNewWords:\n",
    "            # Get the next word or phrase in the lower-cased text with phrases and\n",
    "            # match it to the original form of the same n-gram in the original text\n",
    "            newWord = newWords[newIndex]\n",
    "            phraseWords = regexUnderBar.split(newWord)\n",
    "            numPhraseWords = len(phraseWords)\n",
    "            matchedWords = \" \".join(origWords[origIndex:(origIndex+numPhraseWords)])\n",
    "            origIndex += numPhraseWords\n",
    "                \n",
    "            # Now do the bookkeeping for collecting the different surface form \n",
    "            # variations present for each lowercased word or phrase\n",
    "            if newWord in vocabToSurfaceFormHash:\n",
    "                vocabToSurfaceFormHash[newWord].add(matchedWords)\n",
    "            else:\n",
    "                vocabToSurfaceFormHash[newWord] = set([matchedWords])\n",
    "\n",
    "            # Increment the counter for this surface form\n",
    "            if matchedWords not in surfaceFormCountHash:\n",
    "                surfaceFormCountHash[matchedWords] = 1\n",
    "            else:\n",
    "                surfaceFormCountHash[matchedWords] += 1\n",
    "   \n",
    "            if ( len(newWord) != len(matchedWords)):\n",
    "                print (\"##### Error #####\")\n",
    "                print (\"Bad Match: %s ==> %s \" % (newWord,matchedWords))\n",
    "                print (\"From line: %s\" % textData['TextWithPhrases'][i])\n",
    "                print (\"Orig text: %s\" % textData['CleanedText'][i])\n",
    "                \n",
    "                return False\n",
    "\n",
    "            newIndex += 1\n",
    "    # After aligning and counting, select the most common surface form for each\n",
    "\n",
    "    # word/phrase to be the canonical example shown to the user for that word/phrase\n",
    "    for ngram in vocabToSurfaceFormHash.keys():\n",
    "        maxCount = 0\n",
    "        bestSurfaceForm = \"\"\n",
    "        for surfaceForm in vocabToSurfaceFormHash[ngram]:\n",
    "            if surfaceFormCountHash[surfaceForm] > maxCount:\n",
    "                maxCount = surfaceFormCountHash[surfaceForm]\n",
    "                bestSurfaceForm = surfaceForm\n",
    "        if ngram != \"\":\n",
    "            if bestSurfaceForm == \"\":\n",
    "                print (\"Warning: NULL surface form for ngram '%s'\" % ngram)\n",
    "            else:\n",
    "                vocabToSurfaceFormHash[ngram] = bestSurfaceForm\n",
    "    \n",
    "    return vocabToSurfaceFormHash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if True:\n",
    "    vocabToSurfaceFormHash = MapVocabToSurfaceForms(textFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mapping between model vocabulary and surface form mapping\n",
    "tsvFile = \"../Data/Vocab2SurfaceFormMapping.tsv\"\n",
    "\n",
    "saveSurfaceFormFile = True\n",
    "\n",
    "if saveSurfaceFormFile:\n",
    "    with open(tsvFile, 'w', encoding='utf-8') as fp:\n",
    "        for key, val in vocabToSurfaceFormHash.items():\n",
    "            if key != \"\":\n",
    "                strOut = \"%s\\t%s\\n\" % (key, val)\n",
    "                fp.write(strOut)\n",
    "else:\n",
    "    # Load surface form mappings here\n",
    "    vocabToSurfaceFormHash = {}\n",
    "    fp = open(tsvFile, encoding='utf-8')\n",
    "\n",
    "    # Each line in the file has two tab separated fields;\n",
    "    # the first is the vocabulary item used during modeling\n",
    "    # and the second is its most common surface form in the \n",
    "    # original data\n",
    "    for stringIn in fp.readlines():\n",
    "        fields = stringIn.strip().split(\"\\t\")\n",
    "        if len(fields) != 2:\n",
    "            print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "        elif fields[0] == \"\" or fields[1] == \"\":\n",
    "            print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "        else:\n",
    "            vocabToSurfaceFormHash[fields[0]] = fields[1]\n",
    "    fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n",
      "Declares\n",
      "mental health\n",
      "El Salvador\n",
      "Department of the Interior\n"
     ]
    }
   ],
   "source": [
    "print (vocabToSurfaceFormHash['security'])\n",
    "print (vocabToSurfaceFormHash['declares'])\n",
    "print (vocabToSurfaceFormHash['mental_health'])\n",
    "print (vocabToSurfaceFormHash['el_salvador'])\n",
    "print (vocabToSurfaceFormHash['department_of_the_interior'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the Full Processed Text of Each Document and Put it into a New Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReconstituteDocsFromChunks(textData, idColumnName, textColumnName):\n",
    "    dataOut = []\n",
    "    \n",
    "    currentDoc = \"\"\n",
    "    currentDocID = \"\"\n",
    "    \n",
    "    for i in range(0,len(textData)):\n",
    "        textChunk = textData[textColumnName][i]\n",
    "        docID = str(textData[idColumnName][i])\n",
    "        if docID != currentDocID:\n",
    "            if currentDocID != \"\":\n",
    "                dataOut.append([currentDocID, currentDoc])\n",
    "            currentDoc = textChunk\n",
    "            currentDocID = docID\n",
    "        else:\n",
    "            currentDoc += \" \" + textChunk\n",
    "    dataOut.append([currentDocID,currentDoc])\n",
    "    \n",
    "    frameOut = pandas.DataFrame(dataOut, columns=['DocID','ProcessedText'])\n",
    "    \n",
    "    return frameOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if True:\n",
    "    docsFrame = ReconstituteDocsFromChunks(textFrame, 'DocID', 'TextWithPhrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveProcessedText = True\n",
    "\n",
    "# Save processed text for each document back out to a TSV file\n",
    "if saveProcessedText:\n",
    "    docsFrame.to_csv('../Data/CongressionalDocsProcessed.tsv', sep='\\t', index=False)\n",
    "else: \n",
    "    docsFrame = pandas.read_csv('../Data/CongressionalDocsProcessed.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>provides that effective from january_3 1973 th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>establishes a joint congressional_committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres4-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres5-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DocID                                      ProcessedText\n",
       "0  hconres1-93  provides that effective from january_3 1973 th...\n",
       "1  hconres2-93  makes_it_the_sense_of_the_congress that the po...\n",
       "2  hconres3-93  establishes a joint congressional_committee on...\n",
       "3  hconres4-93  makes_it_the_sense_of_the_congress that the pr...\n",
       "4  hconres5-93  makes_it_the_sense_of_the_congress that the co..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsFrame[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'makes_it_the_sense_of_the_congress that the pollution of waters all over the world is a matter of vital concern to all_nations and should be dealt with as a matter of the highest_priority makes_it_the_sense_of_the_congress that the president acting through the united_states delegation to the united national_conference on the human_environment should take such steps as may be necessary to propose an international_agreement or amendments to existing international_agreements as may be appropriate providing for coordinated international activites to prohibit the disposal of munitions chemicals chemical_munitions military material and any pollutants in territorial_waters contiguous zones the deep_seabed or any international waters and otherwise to prevent the pollution of the waters of the world'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsFrame['ProcessedText'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Apply Rules to New Documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewritesInPlace(textFrame, textColumnName, phraseRules):\n",
    "    \n",
    "    # Make sure we have phrase to add\n",
    "    numPhraseRules = len(phraseRules)\n",
    "    if numPhraseRules == 0: \n",
    "        print (\"Warning: phrase rule lise is empty - no phrases being applied to text data\")\n",
    "        return\n",
    "    \n",
    "    # Get text data column from frame\n",
    "    textData = textFrame[textColumnName]\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # Add leading and trailing spaces to make regex matching easier\n",
    "    for i in range(0,numLines):\n",
    "        textData[i] = \" \" + textData[i] + \" \"  \n",
    "\n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "   \n",
    "    # Initialize some bookkeeping variables\n",
    "\n",
    "    # Iterate through full set of phrases to find sets of \n",
    "    # non-conflicting phrases that can be apply simultaneously\n",
    "    index = 0\n",
    "    outerStop = False\n",
    "    while not outerStop:\n",
    "       \n",
    "        # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "        leftConflictHash = {}\n",
    "        rightConflictHash = {}\n",
    "        prevConflictHash = {}\n",
    "    \n",
    "        # Create an empty hash table collecting the next set of rewrite rules\n",
    "        # to be applied during this iteration of phrase rewriting\n",
    "        phraseRewriteHash = {}\n",
    "    \n",
    "        # Progress through phrases until the next conflicting phrase is found\n",
    "        innerStop = 0\n",
    "        numPhrasesAdded = 0\n",
    "        while not innerStop:\n",
    "        \n",
    "            # Get the next phrase to consider adding to the phrase list\n",
    "            nextPhrase = phraseRules[index]            \n",
    "            \n",
    "            # Extract the left and right sides of the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = nextPhrase.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[-1] \n",
    "\n",
    "            # Stop if we reach any phrases that conflicts with earlier phrases in this iteration\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if ((leftWord in leftConflictHash) or (rightWord in rightConflictHash) \n",
    "                or (leftWord in prevConflictHash) or (rightWord in prevConflictHash)): \n",
    "                innerStop = True\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                # Create the output compound word version of the phrase\n",
    "                                \n",
    "                outputPhrase = regexSpace.sub(\"_\",nextPhrase);\n",
    "                \n",
    "                # Keep track of all context words that might conflict with upcoming\n",
    "                # propose phrases (even when phrases are skipped instead of added)\n",
    "                leftConflictHash[rightWord] = 1\n",
    "                rightConflictHash[leftWord] = 1\n",
    "                prevConflictHash[outputPhrase] = 1           \n",
    "                \n",
    "                # Add extra space to input an output versions of the current phrase \n",
    "                # to make the regex rewrite easier\n",
    "                outputPhrase = \" \" + outputPhrase\n",
    "                lastAddedPhrase = \" \" + nextPhrase\n",
    "                \n",
    "                # Add the phrase to the rewrite hash\n",
    "                phraseRewriteHash[lastAddedPhrase] = outputPhrase\n",
    "                  \n",
    "                # Increment to next phrase\n",
    "                index += 1\n",
    "                numPhrasesAdded  += 1\n",
    "    \n",
    "                # Stop if we've reached the end of the phrases list\n",
    "                if index >= numPhraseRules:\n",
    "                    innerStop = True\n",
    "                    outerStop = True\n",
    "                    \n",
    "        # Now do the phrase rewrites over the entire set of text data\n",
    "        if numPhrasesAdded == 1:\n",
    "        \n",
    "            # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "            outputPhrase = phraseRewriteHash[lastAddedPhrase]\n",
    "            regexPhrase = re.compile (r'%s(?= )' % re.escape(lastAddedPhrase)) \n",
    "        \n",
    "            # Apply the regex over the full data set\n",
    "            for j in range(0,numLines):\n",
    "                textData[j] = regexPhrase.sub(outputPhrase, textData[j])\n",
    "        \n",
    "        elif numPhrasesAdded > 1:\n",
    "            # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "            regexPhrase = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, phraseRewriteHash.keys())))\n",
    "            \n",
    "            # Apply the regex over the full data set\n",
    "            for i in range(0,numLines):\n",
    "                # The regex substituion looks up the output string rewrite  \n",
    "                # in the hash table for each matched input phrase regex\n",
    "                textData[i] = regexPhrase.sub(lambda mo: phraseRewriteHash[mo.string[mo.start():mo.end()]], textData[i]) \n",
    "    \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president_of_the_united_states appoints the secretary_of_labor to lead the department_of_labor\n",
      "the speaker_of_the_house_of_representatives is elected each session by the members_of_the_house\n",
      "the president_pro_tempore of the the u.s. senate resides over the senate when the vice_president is absent\n"
     ]
    }
   ],
   "source": [
    "testText = [\"the president of the united states appoints the secretary of labor to lead the department of labor\", \n",
    "            \"the speaker of the house of representatives is elected each session by the members of the house\",\n",
    "            \"the president pro tempore of the the u.s. senate resides over the senate when the vice president is absent\"]\n",
    "\n",
    "testFrame = pandas.DataFrame(testText, columns=['TestText'])      \n",
    "\n",
    "ApplyPhraseRewritesInPlace(testFrame, 'TestText', learnedPhrases)\n",
    "\n",
    "print(testFrame['TestText'][0])\n",
    "print(testFrame['TestText'][1])\n",
    "print(testFrame['TestText'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
