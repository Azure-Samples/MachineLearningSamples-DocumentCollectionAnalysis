{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Learning of Key Phrases and Topics in Document Collections\n",
    "\n",
    "## Part 1: Text Preprocessing\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is Part 1 in a series of 6, providing a step-by-step description of how to process and analyze the contents of a large collection of text documents in an unsupervised manner. Using Python packages and custom code examples, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.\n",
    "\n",
    "This notebook demonstrates how to preprocess the raw text from a collection of documents as precursor to applying the natural language processing techniques of unsupervised phrase learning and latent topic modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These series of notebooks can be run on any compute context. Before you run those notebooks, make sure you have all dependencies installed in the compute context you choose as kernel.**\n",
    "\n",
    "* For **local** kernel, click \"_**Open Command Prompt**_\" from \"_**File**_\" menu in Azure Machine Learning Workbench, and then manually install the following packages:\n",
    "```\n",
    "$ conda install numpy\n",
    "$ conda install nltk\n",
    "$ conda install -c conda-forge wordcloud\n",
    "$ conda install bokeh\n",
    "$ pip install gensim\n",
    "$ pip install matplotlib\n",
    "```\n",
    "\n",
    "* For local or remote **Docker kernels**:\n",
    "    * ensure **notebook**, **matplotlib**, **nltk**, **gensim**, **wordcloud** are listed in your **conda_dependencies.yml** file under **aml_config** folder.\n",
    "    ```\n",
    "        name: project_environment\n",
    "        channels:\n",
    "          - conda-forge\n",
    "          - defaults\n",
    "        dependencies:\n",
    "          - python=3.5.2\n",
    "          - numpy>=1.13\n",
    "          - scikit-learn\n",
    "          - nltk\n",
    "          - pandas\n",
    "          - azure\n",
    "          - gensim\n",
    "          - scipy\n",
    "          - wordcloud\n",
    "          - bokeh\n",
    "          - pip:\n",
    "            - notebook\n",
    "            - gensim\n",
    "            - matplotlib\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing NLTK Model for Sentence Tokenization\n",
    "\n",
    "\n",
    "NLTK is a collection of Python modules, prebuilt models and corpora that provides tools for complex natural language processing tasks. Because the toolkit is large, the base installation of NLTK only installs the core skeleton of the toolkit. Installation of specific modules, corpora and pre-built models can be invoked from within Python using a download functionality provided by NLTK that can be invoked from Python. \n",
    "\n",
    "In this notebook, we make use of the NLTK sentence tokenization capability which takes a long string of text and splits it into sentence units. The tokenizer requires the installation of the 'punkt'  tokenizer models. After importing nltk, the nltk.download() function can be used to download specific packages such as 'punkt'.\n",
    "\n",
    "For more information on NLTK see http://www.nltk.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x237d04441d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import nltk\n",
    "# The first time you run NLTK you will need to download the 'punkt' models \n",
    "# for breaking text strings into individual sentences\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "from azureml.logging import get_azureml_logger\n",
    "aml_logger = get_azureml_logger()   # logger writes to AMLWorkbench runtime view\n",
    "aml_logger.log('amlrealworld.document-collection-analysis.notebook1', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Other Required Packages\n",
    "The 'pandas' package is used for handling and manipulating data frames. The 're' package is used for applying regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to download the datasets from Blob Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: If you are running this notebook outside of Azure Machine Learning Workbench, you will need to change the `'shared_path'` to relative path `'../Data'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_blob(filename):\n",
    "    shared_path = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "    save_path = os.path.join(shared_path, filename)\n",
    "\n",
    "    # Base URL for anonymous read access to Blob Storage container\n",
    "    STORAGE_CONTAINER = 'https://bostondata.blob.core.windows.net/scenario-document-collection-analysis/'\n",
    "    url = STORAGE_CONTAINER + filename\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "    \n",
    "    \n",
    "def getData():\n",
    "    shared_path = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "    data_file = os.path.join(shared_path, DATASET_FILE)\n",
    "    blacklist_file = os.path.join(shared_path, BLACK_LIST_FILE)\n",
    "    function_words_file = os.path.join(shared_path, FUNCTION_WORDS_FILE)\n",
    "\n",
    "    if not os.path.exists(data_file):\n",
    "        download_file_from_blob(DATASET_FILE)\n",
    "    if not os.path.exists(blacklist_file):\n",
    "        download_file_from_blob(BLACK_LIST_FILE)\n",
    "    if not os.path.exists(function_words_file):\n",
    "        download_file_from_blob(FUNCTION_WORDS_FILE)\n",
    "\n",
    "    df = pandas.read_csv(data_file, sep='\\t')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants used to get data from Blog Storage and read it as Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset file name\n",
    "# DATASET_FILE = 'small_data.tsv'\n",
    "DATASET_FILE = 'CongressionalDataAll_Jun_2017.tsv'\n",
    "\n",
    "# The black list of words to ignore\n",
    "BLACK_LIST_FILE = 'black_list.txt'\n",
    "\n",
    "# The non-content bearing function words\n",
    "FUNCTION_WORDS_FILE = 'function_words.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** By default, this notebook will use the entire congressional dataset which contains about 290,000 bills. There is also an option to comment the line `DATASET_FILE = 'CongressionalDataAll_Jun_2017.tsv'` and uncomment line `DATASET_FILE = 'small_data.tsv'` on the above cell to run it on a dataset with 50,000 bills. Use a small dataset could significantly reduce the execution time of downstream notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load full TSV file including a column of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in corpus: 297462\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>SponsorName</th>\n",
       "      <th>Type</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Party</th>\n",
       "      <th>Subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>Provides that effective from January 3, 1973, ...</td>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>O'Neill, Thomas P., Jr.</td>\n",
       "      <td>rep</td>\n",
       "      <td>MA</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>congress,congressional joint committees,govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>Bennett, Charles E.</td>\n",
       "      <td>rep</td>\n",
       "      <td>FL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>environmental protection,pollution,water resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>Bennett, Charles E.</td>\n",
       "      <td>rep</td>\n",
       "      <td>FL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>congress,congressional joint committees,congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres4-93</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>Collier, Harold R.</td>\n",
       "      <td>rep</td>\n",
       "      <td>IL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Republican</td>\n",
       "      <td>armed forces and national security,missing in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres5-93</td>\n",
       "      <td>Makes it the sense of the Congress that: (1) t...</td>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>Collier, Harold R.</td>\n",
       "      <td>rep</td>\n",
       "      <td>IL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Republican</td>\n",
       "      <td>economics and public finance,federal budgets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               Text        Date  \\\n",
       "0  hconres1-93  Provides that effective from January 3, 1973, ...  1973-01-03   \n",
       "1  hconres2-93  Makes it the sense of the Congress that the po...  1973-01-03   \n",
       "2  hconres3-93  Establishes a Joint Congressional Committee on...  1973-01-03   \n",
       "3  hconres4-93  Makes it the sense of the Congress that the Pr...  1973-01-03   \n",
       "4  hconres5-93  Makes it the sense of the Congress that: (1) t...  1973-01-03   \n",
       "\n",
       "               SponsorName Type State  District       Party  \\\n",
       "0  O'Neill, Thomas P., Jr.  rep    MA       8.0    Democrat   \n",
       "1      Bennett, Charles E.  rep    FL       3.0    Democrat   \n",
       "2      Bennett, Charles E.  rep    FL       3.0    Democrat   \n",
       "3       Collier, Harold R.  rep    IL      10.0  Republican   \n",
       "4       Collier, Harold R.  rep    IL      10.0  Republican   \n",
       "\n",
       "                                            Subjects  \n",
       "0  congress,congressional joint committees,govern...  \n",
       "1  environmental protection,pollution,water resou...  \n",
       "2  congress,congressional joint committees,congre...  \n",
       "3  armed forces and national security,missing in ...  \n",
       "4       economics and public finance,federal budgets  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total documents in corpus: %d\\n\" % len(frame))\n",
    "\n",
    "# Show the first five rows of the data in the frame\n",
    "frame[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the full text of the first three documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provides that effective from January 3, 1973, the joint committee created to make the necessary arrangements for the inauguration of the President-elect and Vice President-elect of the United States on the 20th day of January 1973, is hereby continued and for such purpose shall have the same power and authority as that conferred by Senate Concurrent Resolution 63, of the Ninety-second Congress.\n",
      "---\n",
      "Makes it the sense of the Congress that the pollution of waters all over the world is a matter of vital concern to all nations and should be dealt with as a matter of the highest priority. Makes it the sense of the Congress that the President, acting through the United States delegation to the United National Conference on the Human Environment, should take such steps as may be necessary to propose an international agreement, or amendments to existing international agreements, as may be appropriate, providing for coordinated international activites to prohibit the disposal of munitions, chemicals, chemical munitions, military material, and any pollutants in territorial waters, contiguous zones, the deep seabed or any international waters, and otherwise to prevent the pollution of the waters of the world.\n",
      "---\n",
      "Establishes a Joint Congressional Committee on Impoundment of Funds to exist during the 93rd Congress and to consist of five Members of the Senate appointed by the President pro tempore of the Senate and five Members of the House of Representatives to be appointed by the Speaker of the House of Representatives. States that it shall be the duty of the committee to conduct a continuing comprehensive study and review of the President's constitutional power to terminate authorized Federal projects for which appropriations have been made or to withhold funds from such projects, and to report to the Congress its findings in a final report before the close of the 93rd Congress. Authorizes the committee to make such expenditures, hold such hearings, and employ such assistants as necessary, and to utilize the services and facilities of the General Accounting Office, any executive agency, or any congressional committee.\n"
     ]
    }
   ],
   "source": [
    "print(frame['Text'][0])\n",
    "print('---')\n",
    "print(frame['Text'][1])\n",
    "print('---')\n",
    "print(frame['Text'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text Data\n",
    "\n",
    "The CleanAndSplitText function below takes as input a list where each row element is a single cohesive long string of text, i.e. a \"document\". The function first splits each string by various forms of punctuation into chunks of text that are likely sentences, phrases or sub-phrases. The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.\n",
    "\n",
    "The function creates a table where each row represents a chunk of text from the original documents. The DocIndex column indicates the original row index from associated document in the input from which the chunk of text originated. The TextLine column contains the original text excluding the punctuation marks and HTML markup that have been during the cleaning process.The TextLineLower column contains a fully lower-cased version of the text in the TextLIne column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanAndSplitText(textDataFrame):\n",
    "\n",
    "    textDataOut = [] \n",
    "   \n",
    "    # This regular expression is for section headers in the bill summaries that we wish to ignore\n",
    "    reHeaders = re.compile(r\" *TABLE OF CONTENTS:? *\"\n",
    "                           \"| *Title [IVXLC]+:? *\"\n",
    "                           \"| *Subtitle [A-Z]+:? *\"\n",
    "                           \"| *\\(Sec\\. \\d+\\) *\")\n",
    "\n",
    "    # This regular expression is for punctuation that we wish to clean out\n",
    "    # We also will split sentences into smaller phrase like units using this expression\n",
    "    rePhraseBreaks = re.compile(\"[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]*\\s+\\([0-9]+\\)\\s+[\\(\\[\\{\\\"\\*\\-]*\"                             \n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]+\\s+[\\(\\[\\{\\\"\\*\\-]*\"\n",
    "                                \"|\\.\\.+\"\n",
    "                                \"|\\s*\\-\\-+\\s*\"\n",
    "                                \"|\\s+\\-\\s+\"\n",
    "                                \"|\\:\\:+\"\n",
    "                                \"|\\s+[\\/\\(\\[\\{\\\"\\-\\*]+\\s*\"\n",
    "                                \"|[\\,!\\?\\\"\\)\\(\\]\\[\\}\\{\\:\\;\\*](?=[a-zA-Z])\"\n",
    "                                \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;]+[\\.]*$\"\n",
    "                             )\n",
    "    \n",
    "    # Regex for underbars\n",
    "    regexUnderbar = re.compile('_')\n",
    "    \n",
    "    # Regex for space\n",
    "    regexSpace = re.compile(' +')\n",
    " \n",
    "    # Regex for sentence final period\n",
    "    regexPeriod = re.compile(\"\\.$\")\n",
    "\n",
    "    # Iterate through each document and do:\n",
    "    #    (1) Split documents into sections based on section headers and remove section headers\n",
    "    #    (2) Split the sections into sentences using NLTK sentence tokenizer\n",
    "    #    (3) Further split sentences into phrasal units based on punctuation and remove punctuation\n",
    "    #    (4) Remove sentence final periods when not part of an abbreviation \n",
    "\n",
    "    for i in range(0, len(frame)):     \n",
    "        # Extract one document from frame\n",
    "        docID = frame['ID'][i]\n",
    "        docText = str(frame['Text'][i])\n",
    "\n",
    "        # Set counter for output line count for this document\n",
    "        lineIndex=0;\n",
    "\n",
    "        # Split document into sections by finding sections headers and splitting on them \n",
    "        sections = reHeaders.split(docText)\n",
    "        \n",
    "        for section in sections:\n",
    "            # Split section into sentence using NLTK tokenizer \n",
    "            sentences = tokenize.sent_tokenize(section)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Split each sentence into phrase level chunks based on punctuation\n",
    "                textSegs = rePhraseBreaks.split(sentence)\n",
    "                numSegs = len(textSegs)\n",
    "                \n",
    "                for j in range(0,numSegs):\n",
    "                    if len(textSegs[j])>0:\n",
    "                        # Convert underbars to spaces \n",
    "                        # Underbars are reserved for building the compound word phrases                   \n",
    "                        textSegs[j] = regexUnderbar.sub(\" \",textSegs[j])\n",
    "                    \n",
    "                        # Split out the words so we can specially handle the last word\n",
    "                        words = regexSpace.split(textSegs[j])\n",
    "                        phraseOut = \"\"\n",
    "                        # If the last word ends in a period then remove the period\n",
    "                        words[-1] = regexPeriod.sub(\"\", words[-1])\n",
    "                        # If the last word is an abbreviation like \"U.S.\"\n",
    "                        # then add the word final period back on\n",
    "                        if \"\\.\" in words[-1]:\n",
    "                            words[-1] += \".\"\n",
    "                        phraseOut = \" \".join(words)  \n",
    "\n",
    "                        textDataOut.append([docID, lineIndex, phraseOut])\n",
    "                        lineIndex += 1\n",
    "                        \n",
    "    # Convert to pandas frame \n",
    "    frameOut = pandas.DataFrame(textDataOut, columns=['DocID', 'DocLine', 'CleanedText'])                      \n",
    "    \n",
    "    return frameOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true to run the function\n",
    "writeFile = True\n",
    "\n",
    "if writeFile:\n",
    "    cleanedDataFrame = CleanAndSplitText(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing and reading text data to and from a file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the text data to file and reading it back in. If the value is 'False' it assumes we have already run the CleanAndSplitData function and written it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataFile = os.path.join(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'], 'CongressionalDocsCleaned.tsv')\n",
    "\n",
    "if writeFile:\n",
    "    # Write frame with preprocessed text out to TSV file \n",
    "    cleanedDataFrame.to_csv(cleanedDataFile, sep='\\t', index=False)\n",
    "else:\n",
    "    # Read a cleaned data frame in from a TSV file\n",
    "    cleanedDataFrame = pandas.read_csv(cleanedDataFile, sep='\\t', encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the processed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>1</td>\n",
       "      <td>States that it shall be the duty of the commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>2</td>\n",
       "      <td>and to report to the Congress its findings in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>3</td>\n",
       "      <td>Authorizes the committee to make such expendit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>4</td>\n",
       "      <td>hold such hearings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>5</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText\n",
       "0   hconres1-93        0             Provides that effective from January 3\n",
       "1   hconres1-93        1                                               1973\n",
       "2   hconres1-93        2  the joint committee created to make the necess...\n",
       "3   hconres1-93        3  is hereby continued and for such purpose shall...\n",
       "4   hconres1-93        4                      of the Ninety-second Congress\n",
       "5   hconres2-93        0  Makes it the sense of the Congress that the po...\n",
       "6   hconres2-93        1  Makes it the sense of the Congress that the Pr...\n",
       "7   hconres2-93        2  acting through the United States delegation to...\n",
       "8   hconres2-93        3  should take such steps as may be necessary to ...\n",
       "9   hconres2-93        4  or amendments to existing international agreem...\n",
       "10  hconres2-93        5                              as may be appropriate\n",
       "11  hconres2-93        6  providing for coordinated international activi...\n",
       "12  hconres2-93        7                                          chemicals\n",
       "13  hconres2-93        8                                 chemical munitions\n",
       "14  hconres2-93        9                                  military material\n",
       "15  hconres2-93       10           and any pollutants in territorial waters\n",
       "16  hconres2-93       11                                   contiguous zones\n",
       "17  hconres2-93       12        the deep seabed or any international waters\n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...\n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...\n",
       "20  hconres3-93        1  States that it shall be the duty of the commit...\n",
       "21  hconres3-93        2  and to report to the Congress its findings in ...\n",
       "22  hconres3-93        3  Authorizes the committee to make such expendit...\n",
       "23  hconres3-93        4                                 hold such hearings\n",
       "24  hconres3-93        5            and employ such assistants as necessary"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedDataFrame[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provides that effective from January 3\n",
      "1973\n",
      "the joint committee created to make the necessary arrangements for the inauguration of the President-elect and Vice President-elect of the United States on the 20th day of January 1973\n"
     ]
    }
   ],
   "source": [
    "print(cleanedDataFrame['CleanedText'][0])\n",
    "print(cleanedDataFrame['CleanedText'][1])\n",
    "print(cleanedDataFrame['CleanedText'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "The data preprocessing step is finished. The next step will be phrase learning which will be in the second notebook of the series: [`2_Phrase_Learning.ipynb`](./2_Phrase_Learning.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Document-Collection-Analysis local",
   "language": "python",
   "name": "document-collection-analysis_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
